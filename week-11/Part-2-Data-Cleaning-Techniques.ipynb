{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8078219-dac2-4abc-a9f6-1a61818b81d4",
   "metadata": {},
   "source": [
    "##  Data Cleaning Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807de7ec-7260-46ca-a7cd-43b36ff47569",
   "metadata": {},
   "source": [
    "The previous section was dedicated to identifying errors and quality issues in the data. Now we finally get to do something about it! In this section, we'll step through some basic approaches you can take to address the issues we identified in the web-scraped LinkedIn dataset. Growing your data preparation skills will be an ongoing effort throughout your career, so let's get started. \n",
    "  \n",
    "The scenario for this section is a continuation of the work done for the previous section. To review, you have been asked by the head of recruiting for a tech company to analyze the data science jobs that are being advertised on LinkedIn to predict the most competitive salary bands and bonuses the company should be offering to employees and new hires to attract/retain talent. \n",
    "  \n",
    "In this section, we are going to use pandas and matplotlib, so weâ€™ll start by importing those libraries using aliases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a614dab-ca99-47c3-bd81-81ef5fa98a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7689e4",
   "metadata": {},
   "source": [
    "Let's import the LinkedIn dataset as `jobs`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93e0a61c-228f-4e5f-8002-5beb97448eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>description</th>\n",
       "      <th>onsite_remote</th>\n",
       "      <th>salary</th>\n",
       "      <th>sign_on_bonus</th>\n",
       "      <th>annual_bonus</th>\n",
       "      <th>location</th>\n",
       "      <th>criteria</th>\n",
       "      <th>posted_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Analyst - Recent Graduate</td>\n",
       "      <td>paypal</td>\n",
       "      <td>At PayPal (NASDAQ: PYPL), we believe that ever...</td>\n",
       "      <td>onsite</td>\n",
       "      <td>$75,000.00\\n            -\\n            $95,000.00</td>\n",
       "      <td>9000</td>\n",
       "      <td>2400.0</td>\n",
       "      <td>Buffalo-Niagara Falls Area</td>\n",
       "      <td>[{'Seniority level': 'Not Applicable'}, {'Empl...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Analyst - Recent Graduate</td>\n",
       "      <td>paypal</td>\n",
       "      <td>At PayPal (NASDAQ: PYPL), we believe that ever...</td>\n",
       "      <td>onsite</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4000</td>\n",
       "      <td>5400.0</td>\n",
       "      <td>San Jose, CA</td>\n",
       "      <td>[{'Seniority level': 'Not Applicable'}, {'Empl...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>paypal</td>\n",
       "      <td>At PayPal (NASDAQ: PYPL), we believe that ever...</td>\n",
       "      <td>onsite</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>Texas, United States</td>\n",
       "      <td>[{'Seniority level': 'Not Applicable'}, {'Empl...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>PayPal</td>\n",
       "      <td>At PayPal (NASDAQ: PYPL), we believe that ever...</td>\n",
       "      <td>onsite</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3000</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>Illinois, United States</td>\n",
       "      <td>[{'Seniority level': 'Not Applicable'}, {'Empl...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Entry-Level Data Analyst</td>\n",
       "      <td>The Federal Savings Bank</td>\n",
       "      <td>The Federal Savings Bank, a national bank and ...</td>\n",
       "      <td>onsite</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4000</td>\n",
       "      <td>2400.0</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>[{'Seniority level': 'Entry level'}, {'Employm...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            title                   company  \\\n",
       "0  Data Analyst - Recent Graduate                    paypal   \n",
       "1  Data Analyst - Recent Graduate                    paypal   \n",
       "2                    Data Analyst                    paypal   \n",
       "3                    Data Analyst                    PayPal   \n",
       "4        Entry-Level Data Analyst  The Federal Savings Bank   \n",
       "\n",
       "                                         description onsite_remote  \\\n",
       "0  At PayPal (NASDAQ: PYPL), we believe that ever...        onsite   \n",
       "1  At PayPal (NASDAQ: PYPL), we believe that ever...        onsite   \n",
       "2  At PayPal (NASDAQ: PYPL), we believe that ever...        onsite   \n",
       "3  At PayPal (NASDAQ: PYPL), we believe that ever...        onsite   \n",
       "4  The Federal Savings Bank, a national bank and ...        onsite   \n",
       "\n",
       "                                              salary  sign_on_bonus  \\\n",
       "0  $75,000.00\\n            -\\n            $95,000.00           9000   \n",
       "1                                                NaN           4000   \n",
       "2                                                NaN           2000   \n",
       "3                                                NaN           3000   \n",
       "4                                                NaN           4000   \n",
       "\n",
       "   annual_bonus                    location  \\\n",
       "0        2400.0  Buffalo-Niagara Falls Area   \n",
       "1        5400.0                San Jose, CA   \n",
       "2        1200.0        Texas, United States   \n",
       "3        1800.0     Illinois, United States   \n",
       "4        2400.0                 Chicago, IL   \n",
       "\n",
       "                                            criteria posted_date  \n",
       "0  [{'Seniority level': 'Not Applicable'}, {'Empl...         NaN  \n",
       "1  [{'Seniority level': 'Not Applicable'}, {'Empl...         NaN  \n",
       "2  [{'Seniority level': 'Not Applicable'}, {'Empl...         NaN  \n",
       "3  [{'Seniority level': 'Not Applicable'}, {'Empl...         NaN  \n",
       "4  [{'Seniority level': 'Entry level'}, {'Employm...         NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the CSV file into a pandas DataFrame \n",
    "jobs = pd.read_csv(\"../datasets/li-jobs-usa.csv\")\n",
    "\n",
    "# display the first five rows\n",
    "jobs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a1ddc6-7502-4b5e-bc4c-a71a29ce13b9",
   "metadata": {},
   "source": [
    "### Part I. Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cd2773",
   "metadata": {},
   "source": [
    "With variables with high percentages of missing values, we have several options. Some basic ones are: remove the variable, replace missing values with a reasonable value given the data or with a notation like \"Unknown\", or remove the observations (rows) which are missing data. The choice depends on how much data is missing and the importance of the variable for your analysis. \n",
    " \n",
    "Let's review the percentage of missing values from the columns in the `jobs` dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b92987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display percentage of missing values for each column\n",
    "jobs.isnull().mean()*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c1eb4a",
   "metadata": {},
   "source": [
    "<b>Case 1: `posted_date` </b>\n",
    "Most of the observations in the `jobs` dataset are missing values for this variable, and it isn't important to our analysis, so we'll remove it using the `drop()` method: \n",
    "\n",
    "`df_name.drop(columns=[column 1, ...column n], inplace=[True|False])` \n",
    "\n",
    "\n",
    "The `columns` parameter takes a list of column names that will be removed from the DataFrame. The `inplace` parameter takes a boolean value specifying whether to change the current DataFrame in place or not. So, let's remove the `posted_date` column:  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce8c58b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# drop column(s)\n",
    "jobs.drop(columns=['posted_date'],inplace=True)\n",
    "\n",
    "# display the first five rows\n",
    "jobs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed2db8d",
   "metadata": {},
   "source": [
    "<b>Case 2: `salary` </b>\n",
    "The `salary` variable also has a high percentage of missing data, but will be very important to the predictive work we are prepping this data for. So instead of removing the entire column, we will only remove the rows which contain missing values using the `dropna()` method: \n",
    "\n",
    "`df_name.dropna(subset=[column 1, ...column n], inplace=[True|False])` \n",
    "\n",
    "\n",
    "The `subset` parameter takes a list of column names that let the `dropna()` method know which rows should be removed based on missing values on those columns. Let's remove the rows which contain missing values in the `salary` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51307c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows with missing values in particular column\n",
    "jobs.dropna(subset=['salary'], inplace=True)\n",
    "\n",
    "# display the first five rows\n",
    "jobs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54955ed",
   "metadata": {},
   "source": [
    "<b>Case 3: `annual_bonus` </b>\n",
    "\n",
    "This variable is important to our analysis since it is about compensation, and it is missing less than 1% of its data. So, we will replace the missing values with reasonable estimates using imputation. Imputation means replacing the missing data with reasonable estimates given the data so that later data processing tasks can still work with the complete dataset. Commonly used reasonable estimates are: the average, median, or most frequent value of a column. \n",
    "\n",
    "The method to replace missing values in pandas is the `fillna()` method: \n",
    "\n",
    "`df_name[\"column_name\"].fillna(replacement_value, inplace=[True|False])` \n",
    "\n",
    "We'll use the average value of the `annual_bonus` column to fill in the missing values: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd8baa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the average of a column into a variable\n",
    "avg = jobs['annual_bonus'].mean()\n",
    "\n",
    "# replace missing values with specified value\n",
    "jobs['annual_bonus'].fillna(avg, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e51db2",
   "metadata": {},
   "source": [
    "<b>Case 4: `location` </b>\n",
    "\n",
    "Lastly, we have the `location` column that is not numerical and is missing values. Replacing the missing values with the mean is not an option for categorical variables. Instead, we'll replace the data with a placeholder value that can be filtered later. We'll use the word `Unknown` to replace the missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91e46d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace missing values with replacement value\n",
    "jobs['location'].fillna('Unknown', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0921dfe4-c658-4e94-8080-6213358242a7",
   "metadata": {},
   "source": [
    "### Part II. Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95d10af",
   "metadata": {},
   "source": [
    "Outliers can have more significant implications on the data than what we've previously discussed about missing values. For instance, an outlier might be an unexpected yet plausible data point, unquestionably part of the data set. Alternatively, an outlier might significantly deviate from the rest of the data, possibly indicating experimental errors or mistakes. Therefore, the cause and meaning of an outlier should be thoroughly investigated. Depending on the findings of this investigation, various actions can be taken.  \n",
    "\n",
    "Let's revisit the box plot from the previous section, which depicts the outliers in the `sign_on_bonus` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b963fe56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a boxplot \n",
    "plt.boxplot(jobs['sign_on_bonus'])\n",
    "\n",
    "# annotations to help with identifying outliers\n",
    "plt.annotate('outliers', xy=(.97, 9000), xytext=(.65, 8000), arrowprops=dict(facecolor='black', width=1))\n",
    "plt.annotate('outliers', xy=(.97, 8000), xytext=(.65, 8000), arrowprops=dict(facecolor='black', width=1))\n",
    "\n",
    "# modify the values of the ticks\n",
    "plt.xticks([1], ['sign_on_bonus'])\n",
    "\n",
    "# label y-axis\n",
    "plt.ylabel(\"USD\")\n",
    "\n",
    "# display the figure \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bd9b25",
   "metadata": {},
   "source": [
    "The first thing and, above all, easiest thing to do would be to just leave the data as it is in the dataset! Since the `sign_on_bonus` column represents sign-on bonuses given to new employees for accepting an offer, high sign-on bonuses are a fact of reality that depends on the company, subject expertise, and/or years of experience, among a number of other factors.  \n",
    "\n",
    "However, since there are only two outliers here, this means that only 0.072% of the data is an outlier. An incredibly small number! Thus, we are going to replace the outlier values with something more central to the distribution.  \n",
    "\n",
    "The following code replaces both outlier values with the max value of the distribution without outliers by first finding the index values of the rows which contain the outliers and then directly assigning them the new max value:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f704b6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign max value of distribution without the outliers\n",
    "max_value = jobs[jobs['sign_on_bonus'] < 7000].sign_on_bonus.max()\n",
    "\n",
    "# assign indices of rows which needs to be replaced (the rows with the outliers)\n",
    "indices = jobs[jobs['sign_on_bonus'] > 7000].index\n",
    "\n",
    "# replace the outlier values with replacement value\n",
    "jobs.loc[indices,'sign_on_bonus'] = max_value\n",
    "\n",
    "# verify that the values have been changed\n",
    "jobs.loc[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934fa973",
   "metadata": {},
   "source": [
    "Note that the replacement value could have been something like the average, like the missing values section, but, in this case, the max is closer to the outliers' value than the average would be so using the max changes the dataset less than using the average would.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cbe2f7",
   "metadata": {},
   "source": [
    "### PI 5.3.4 Unnecessary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4d8e7c",
   "metadata": {},
   "source": [
    "Unlike the prior two sections where missing values or outliers could be substituted, dealing with unnecessary data is more direct, essentially involving its removal from the dataset. It's crucial to remember that the only data that should remain in the dataset is that which is relevant and adds value.\n",
    "\n",
    "To that end, we revisit the number of values found within the `onsite_remote` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9612ed92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# output the frequency of value occurrences in column\n",
    "jobs['onsite_remote'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c02b7e",
   "metadata": {},
   "source": [
    "Given the low variation in data, the utility of this data for training a predictive model greatly depends on the specific objective. Even though we can calculate the frequency of occurrences from data that isn't \"balanced\" or equally distributed, like the 'onsite_remote' column, a high-frequency value like 'onsite' could potentially introduce a bias into the model. \n",
    "\n",
    "To remove the unnecessary column, once again, we'll use the `drop()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e102062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop column(s)\n",
    "jobs.drop(columns=['onsite_remote'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a318585",
   "metadata": {},
   "source": [
    "##### Duplicate Rows\n",
    "\n",
    "Remember that another problem we found with the LinkedIn dataset was duplicated rows. \n",
    "\n",
    "We don't need slicing or indexing to remove duplicate rows. Instead, pandas provides us with the handy `drop_duplicates()` method:\n",
    "\n",
    "`df_name.drop_duplicates(inplace=[True|False])`\n",
    "\n",
    "Let's remove the duplicates in our LinkedIn dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcbd28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output all the rows which are duplicated\n",
    "jobs[jobs.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f182ba39",
   "metadata": {},
   "source": [
    "No slicing or indexing is needed to actively remove those duplicate rows as pandas provide the `drop_duplicates()` method. The `drop_duplicates()` method is used as follows:\n",
    "\n",
    "`df_name.drop_duplicates(inplace=[True|False])`\n",
    "\n",
    "Let's remove the duplicates in our LinkedIn dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8c1be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows that are duplicated\n",
    "jobs.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2d4d54",
   "metadata": {},
   "source": [
    "### PI 5.3.5 Inconsistent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d207daa",
   "metadata": {},
   "source": [
    "The best course of action with unnecessary data is to simply remove it. On the other hand, when dealing with inconsistent data, the goal is to modify it in such a way that it becomes consistent. This ensures that the information contained within the inconsistent data remains unaltered following the manipulation.\n",
    "\n",
    "Let's review the values for the `company` column in the LinkedIn dataset that have inconsistencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf328110",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add the casefolded version of the variable you want to check for inconsistencies to the DataFrame\n",
    "jobs['company_casefolded'] = jobs['company'].str.casefold()\n",
    "\n",
    "# Compare the original and the casefolded versions\n",
    "jobs['Is_Inconsistent_company'] = jobs['company'] != jobs['company_casefolded']\n",
    "\n",
    "# Identify the False values since we want company names to be capitalized.\n",
    "jobs[~jobs[\"Is_Inconsistent_company\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219ecb85",
   "metadata": {},
   "source": [
    "Let's isolate which companies have lower case names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4e8a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output unique companies with lower case names\n",
    "jobs[~jobs[\"Is_Inconsistent_company\"]][\"company\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c6f890",
   "metadata": {},
   "source": [
    "We see that there are three of them. Now we need to decide which ones to replace. A little web searching reveals that PayPal and Alice + Olivia are capitalized, but fluent360 is not. So we'll replace all the lower case \"paypal\" and \"alice + olivia\" instances with \"PayPal\" and \"Alice + Olivia\", but leave \"fluent360\" alone using the `str.replace()` method like this: \n",
    "\n",
    "`df_name[\"column_name\"] = df_name[\"column_name\"].str.replace(\"value_to_replace\", \"replacement_value\")`\n",
    "\n",
    "\n",
    "Unlike some of the other replacement methods we've used, the `str.replace()` method does not include an `inplace` parameter. For this method, we apply the `str.replace()` method to a column and overwrite the old column with the new column with all the replacements. \n",
    "\n",
    "\n",
    "Since we need to replace more than one value in our LinkedIn dataset, we'll chain the methods together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef10101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the column values to match preferred case\n",
    "jobs['company'] = jobs['company'].str.replace('paypal','PayPal').str.replace( 'alice + olivia' ,'Alice + Olivia')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcfe459",
   "metadata": {},
   "source": [
    "Now let's revisit the issues we identified with the `salary` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4679319-59d0-4e2f-ace8-7efb6d1db220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the CSV file into a pandas DataFrame \n",
    "jobs = pd.read_csv(\"../datasets/li-jobs-usa.csv\")\n",
    "\n",
    "# display the first five rows\n",
    "jobs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d640d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display first five rows of value counts\n",
    "jobs['salary'].value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36932b3f",
   "metadata": {},
   "source": [
    "Issues:\n",
    "\n",
    "- The column name, \"salary\", might imply a single, yearly salary figure, but instead, this appears to be a salary range column.\n",
    "\n",
    "- There are special characters like `\\n` and lots of whitespace between the values which correspond to carriage returns and newlines (this could be due to the web-scraping method used).\n",
    "\n",
    "- The widely varying values for salary indicate that some of the entries are giving salaries per hour while others are giving yearly figures.\n",
    "\n",
    "\n",
    "#### So what do we do? \n",
    "\n",
    "\n",
    "We'll turn  `salary` into two separate numerical columns, `salary_min` and `salary_max,` with annual, not hourly, salary figures. \n",
    "\n",
    "In other words, we'll turn observations in our current salary column that look like this: \n",
    "\n",
    "`$50.00\\n            -\\n            $60.00` \n",
    "\n",
    "into two columns: \n",
    "\n",
    "`salary min: 100000` and `salary max: 120000`\n",
    "\n",
    "This takes a few steps. \n",
    "\n",
    "First, we'll remove the characters in the `salary` column that are not consistent with a numerical representation of monetary values, namely `$` and `,`, using our familiar `str.replace` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c423806d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace characters with empty string in column values\n",
    "jobs['salary'] = jobs['salary'].str.replace('$','',regex=False)\n",
    "jobs['salary'] = jobs['salary'].str.replace(',','',regex=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59d0067",
   "metadata": {},
   "source": [
    "The next step is to break apart the `salary` column into two columns. We do this we parse the string values representing the salary range by using the `str.split()` method like this:\n",
    "\n",
    "`new_df_name = df_name[column_name].str.split(\"separator\" , expand=[True|False])` \n",
    "\n",
    "If the `expand` parameter is `True`, then the return value is a DataFrame where each column is one of the partitioned strings parsed out according to the separator. \n",
    "\n",
    "Let's see what this looks like before we apply it to the `jobs` DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eadca43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# split the strings in the column into a DataFrame\n",
    "split_string_salaries = jobs['salary'].str.split('-', expand=True)\n",
    " \n",
    "# display DataFrame\n",
    "split_string_salaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eca06f8",
   "metadata": {},
   "source": [
    "Now, we create two new columns in the `jobs` DataFrame based on the `split_string_salaries` DataFrame and drop the original `salary` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc4499d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new columns based on split string\n",
    "jobs['salary_min'] = split_string_salaries[0]\n",
    "jobs['salary_max'] = split_string_salaries[1]\n",
    " \n",
    "# drop unnecessary column\n",
    "jobs.drop(columns=['salary'], inplace=True)\n",
    " \n",
    "# display first five rows\n",
    "jobs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735106bd",
   "metadata": {},
   "source": [
    "Now we need to turn these new columns from strings into numbers so we can compute the annual salary based on those values per hour. We'll use the familiar `astype()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd16160a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change datatypes to float for computation\n",
    "jobs['salary_min'] = jobs['salary_min'].astype(float)\n",
    "jobs['salary_max'] = jobs['salary_max'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c44a16a",
   "metadata": {},
   "source": [
    "Finally, we want all our salary values as annual values. So, how do we know which ones they are? A quick way to visualize the divide between per-hour salaries and annual salaries is to use a histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a606cc67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create histogram plot\n",
    "plt.hist(jobs['salary_min'],bins=20)\n",
    "\n",
    "# label x-axis\n",
    "plt.xlabel('USD')\n",
    "\n",
    "# label y-axis\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# display figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05892566",
   "metadata": {},
   "source": [
    "The large gap between 20,000 and 45,000 tells us that anything in the first bin is a salary per hour value. The next step is to create a filter to isolate all the rows with salary per hour values so that we can convert them. We'll select all `jobs['salary_min']`values less than 5000 USD:  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1512ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign filter for data with hourly wages\n",
    "hourly_wage_filter = (jobs['salary_min'] < 5000)\n",
    " \n",
    "# get indices for rows with hourly wages\n",
    "jobs_with_hourly_wage = jobs[hourly_wage_filter]\n",
    " \n",
    "# verify our values before the change\n",
    "jobs[hourly_wage_filter]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47404f7f",
   "metadata": {},
   "source": [
    "We'll use the `loc[]` method to grab the values and convert them to annual figures by multiplying the hourly wage by 40 x 52 (a 40-hour work week with 52 weeks in a year):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767a93cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace salary values with annual salary (hourly * 40 hours * 52 weeks) \n",
    "jobs.loc[hourly_wage_filter, 'salary_min'] = jobs_with_hourly_wage['salary_min'] * 2080\n",
    "jobs.loc[hourly_wage_filter, 'salary_max'] = jobs_with_hourly_wage['salary_max'] * 2080\n",
    "\n",
    "# verify that values have been changed\n",
    "jobs[hourly_wage_filter]"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m87"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
